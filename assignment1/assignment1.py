# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O6QK2wESSdTRb-jjMGZORZ-Gy6V5ksK5
"""

from google.colab import drive
import os

drive.mount('/content/gdrive')
dataset_path = "gdrive/My Drive/Colab Notebooks/DEEP LEARNING/"

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt

data = pd.read_csv(dataset_path + "iris.data")
data

(data['class']).nunique()  #number of classes = 3, number of instances= 150, number of features = 4
(data['x4']).nunique()

plt.hist(data['x1'], 20, facecolor='blue')
plt.show()

plt.hist(data['x3'], 20, facecolor='blue')
plt.show()

plt.hist(data['x3'], 20, facecolor='blue')
plt.show()

plt.hist(data['x4'], 20, facecolor='blue')
plt.show()

def loss_function(W,X,Y,lambd=0):
  loss = np.dot(-Y.T, np.log(sigmoid(X.dot(W.T))))- np.dot((1-Y).T ,np.log(1 -sigmoid(X.dot(W.T)))) + (lambd/2)*np.dot(W, W.T)
  return loss

def sigmoid(z): 
  return 1.0/(1+np.exp(-z))

def gradient_descent(W, X, Y, learning_rate, lambd=0):
  N = X.shape[0]
  z = X.dot(W.T)
  Y_hat = sigmoid(z)
  grad = (1/N)*(np.dot(X.T, (Y_hat-Y))) + (lambd)*(W.T)
  W = W - learning_rate*grad.T

  return W

"""# Initialization data"""

data  = data.replace('Iris-versicolor', 1)
data = data.replace('Iris-setosa', 0)
def combination_fetures(x1,x2): 
  X= data.iloc[0:100, [x1,x2]].values
  Y = data.iloc[0:100, 4].values
  Y = (Y.reshape(100, 1))
  Y= Y.astype(np.float64)
  np.random.seed(0)
  W= np.random.normal(loc= 0.0, scale= 0.01, size=(1, 3))
  X0 = np.ones((100,1))
  X = np.concatenate((X0, X), axis = 1)
  learning_rate = [0.001,0.01,0.1,0.5,1]
  iterations = 10000

  ax_without_reg = plt.subplot()

  for learning in learning_rate:
    loss_without_reg = []
    for i in range(iterations):
      
      #No Regularization
      loss_without_reg.append(float(loss_function(W, X, Y)))
      W = gradient_descent(W, X, Y, learning)
      

    plt.title("No regularization")
    ax_without_reg.plot(np.arange(iterations),loss_without_reg, label = "learning rate " + str(learning))
    ax_without_reg.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

combination_fetures(2,3)

data  = data.replace('Iris-versicolor', 1)
data = data.replace('Iris-setosa', 0)
X= data.iloc[0:100, [0,2]].values
Y = data.iloc[0:100, 4].values
Y = (Y.reshape(100, 1))
Y= Y.astype(np.float64)
np.random.seed(0)
W= np.random.normal(loc= 0.0, scale= 0.01, size=(1, 3))
X0 = np.ones((100,1))
X = np.concatenate((X0, X), axis = 1)
learning_rate = [0.001,0.01,0.1,0.5,1]
iterations = 10000
lambd = 0.003
ax_reg = plt.subplot()

for learning in learning_rate:
  loss_reg = []
  for i in range(iterations):
    
    #With Regularization
    W = gradient_descent(W, X, Y, learning, lambd)
    loss_reg.append(float(loss_function(W, X, Y, lambd)))

  plt.title("Regularization")
  ax_reg.plot(np.arange(iterations),loss_reg, label = "learning rate " + str(learning))
  ax_reg.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

